#!/usr/bin/env python3

# -*- encoding: utf-8 -*-

import math
import os
import signal
import subprocess
from typing import Any

import cv2 as cv
import numpy as np
import open3d as o3d
import pyransac3d as pyrsc
import rospy
from cv_bridge import CvBridge
from geometry_msgs.msg import Point, Pose2D
from message_filters import ApproximateTimeSynchronizer
from sensor_msgs.msg import CameraInfo, Image
from termcolor import colored

from stalk_detect_vision.config import (BEST_STALK_ALGO, CAMERA_INFO, DEPTH_SCALE,
                                 DEPTH_TOPIC, DEPTH_TRUNC, DRIVER_COMMAND,
                                 GRASP_POINT_ALGO, GRIPPER_LENGTH_PAST_STALK,
                                 GRIPPER_WIDTH, IMAGE_TOPIC, INLIER_THRESHOLD,
                                 MAX_RANSAC_ITERATIONS, MINIMUM_MASK_AREA,
                                 RUN_REALSENSE_ON_REQUEST, VISUALIZE,
                                 BestStalkOptions, GraspPointFindingOptions)
from stalk_detect_vision.model import MaskRCNN
from stalk_detect_vision.srv import GetStalkVision, GetStalkVisionRequest, GetStalkVisionResponse
from stalk_detect_vision.transforms import TfBuffer, Transformer
from stalk_detect_vision.utils import KillableSubscriber, Stalk
from stalk_detect_vision.visualize import Visualizer


class DetectNode:
    @classmethod
    def __init__(cls):
        cls.model = MaskRCNN()
        cls.cv_bridge = CvBridge()

        cls.visualizer = Visualizer()

        cls.tf_buffer = TfBuffer()

        cls.get_stalk_service = rospy.Service('get_stalk_vision', GetStalkVision, cls.find_stalk)

        try:
            rospy.wait_for_message(CAMERA_INFO, CameraInfo, timeout=5)
        except rospy.ROSException:
            print(colored('Camera info not found, so camera is likely not running!', 'yellow'))

        # Count the number of total service calls (for visualization, etc)
        cls.call_index = -1

        # Count the number of images processed in the current service call
        cls.image_index = -1

        cls.driver_process = None

    @classmethod
    def run_detection(cls, image):
        '''
        Run the Mask R-CNN model on the given image

        Parameters
            image (sensor_msgs.msg.Image): The image to run the model on

        Returns
            masks (np.ndarray): The masks of the detected stalks
            output (np.ndarray): The output image of the model
            scores (np.ndarray): The scores of the detected stalks
        '''
        cv_image = cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')

        # Run the model
        scores, bboxes, masks, output = cls.model.forward(cv_image)
        masks = masks.astype(np.uint8) * 255

        return masks, output, scores

    @classmethod
    def get_features(cls, masks) -> 'list[list[Pose2D]]':
        '''
        Get the center points going up each stalk

        Parameters
            masks (np.ndarray): The masks of the detected stalks

        Returns
            stalks_features (list[list[Pose2D]]): The center points of the stalks, for each stalk
        '''
        # Get the center points of the stalks
        stalks_features = []
        for mask in masks:
            # Ensure the mask has the minimum number of pixels
            if np.count_nonzero(mask) < MINIMUM_MASK_AREA:
                continue

            # Swap x and y in the mask
            mask = np.swapaxes(mask, 0, 1)
            nonzero = np.nonzero(mask)

            # Get the top and bottom height values of the stalk
            top_y, bottom_y = nonzero[1].min(), nonzero[1].max()

            stalk_features = [Pose2D(x=np.nonzero(mask[:, top_y])[0].mean(), y=top_y)]

            # For every 10 pixels, get the center point
            for y in range(top_y, bottom_y, 10):
                # Find the average x value for nonzero pixels at this y value
                y_values = np.nonzero(mask[:, y])[0]

                # If there are no y pixels, simply skip this value
                if len(y_values) > 0:
                    stalk_features.append(Pose2D(x=y_values.mean(), y=y))

            stalk_features.append(Pose2D(x=np.nonzero(mask[:, bottom_y])[0].mean(), y=bottom_y))

            stalks_features.append(stalk_features)

        return stalks_features

    @classmethod
    def determine_best_stalk(cls, positions: 'list[Point]') -> Point:
        '''
        Find the best stalk from a list of positions.

        Parameters
            positions (list): A list of positions to cluster and average

        Returns
            (geometry_msgs.msg.Point): The best grasping point
        '''
        def distance(point1: Point, point2: Point):
            return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2)

        # Only store the X and Y values of the positions
        # clustering = OPTICS(eps=0.1, metric='euclidean').fit([[position.x, position.y] for position in positions])
        THRESHOLD = 0.1
        clustering_labels = [0]
        for position in positions[1:]:
            min_distance = float('inf')
            min_cluster = 0
            for i in range(len(clustering_labels)):
                if distance(position, positions[i]) < min_distance:
                    min_distance = distance(position, positions[i])
                    min_cluster = clustering_labels[i]

            clustering_labels.append(min_cluster if min_distance < THRESHOLD else max(clustering_labels) + 1)

        clustering_labels = np.array(clustering_labels)

        # Find the cluster with the most points
        best_cluster = max(set(clustering_labels), key=lambda cluster: np.count_nonzero(clustering_labels == cluster))

        # Average the points in the best cluster
        best_cluster_points = np.array(positions)[np.nonzero(clustering_labels == best_cluster)]
        best_cluster_average = Point(x=np.mean([point.x for point in best_cluster_points]),
                                     y=np.mean([point.y for point in best_cluster_points]))

        # Find the point in the best cluster closest to the average
        best_grasp_point = min(best_cluster_points, key=lambda point: distance(point, best_cluster_average))

        return best_grasp_point

    @classmethod
    def get_pcl(cls, image, depth_image, transformer):
        '''
        Get the point cloud

        Parameters
            image (sensor_msgs.msg.Image): The depth image

        Returns
            point_cloud (pcl PointCloud): The point cloud
        '''
        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
            o3d.geometry.RGBDImage.create_from_color_and_depth(
                o3d.geometry.Image(cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')),
                o3d.geometry.Image(cls.cv_bridge.imgmsg_to_cv2(depth_image, desired_encoding='mono8')),
                depth_scale=DEPTH_SCALE, depth_trunc=DEPTH_TRUNC, convert_rgb_to_intensity=False),
            o3d.camera.PinholeCameraIntrinsic(
                transformer.width, transformer.height, transformer.intrinsic))

        return pcd

    @classmethod
    def transform_points_to_world(cls, stalks_features: 'list[list[Point]]', transformer: Transformer) -> 'list[list[Point]]':
        '''
        Transform the stalk features to the ground plane (using the robot base transform)

        Parameters
            stalk_features (list[list[Point]]): The center points of the stalks
            transformer (Transformer): The transformer

        Returns
            transformed_features (list[list[Point]]): The transformed features
        '''
        transformed_features = []
        for stalk in stalks_features:
            transformed_features.append([transformer.transform_cam_to_world(p) for p in stalk])

        return transformed_features

    @classmethod
    def ransac_ground_plane(cls, pointcloud):
        '''
        Find the ground plane using RANSAC

        Parameters
            pointcloud (o3d.geometry.PointCloud): The point cloud

        Returns
            plane (np.ndarray): The plane coefficients [A,B,C,D] for Ax+By+Cz+D=0
        '''
        # NOTE: Alternatively, use pointcloud.segment_plane from open3d

        A = 150
        B = 100
        C = 10
        colors = np.asarray(pointcloud.colors)
        # Find the brown points in the point cloud
        brown_points = np.array(pointcloud.points)[np.argwhere(np.logical_and(
            colors[2] < A, np.logical_and(abs(colors[0] - colors[1]) < B, np.maximum(colors[0], colors[1]) > C)))]

        # Find the plane using RANSAC
        plane = pyrsc.Plane()
        best_eq, best_inliers = plane.fit(brown_points, INLIER_THRESHOLD, MAX_RANSAC_ITERATIONS)

        return best_eq

    @classmethod
    def get_grasp_points_by_ransac(cls, stalks_features, pointcloud):
        '''
        Find a point closest to 1-3" from the ground plane for each stalk

        Parameters
            stalks_features (list[np.ndarray[Point]]): The center points of the stalks
            pointcloud (o3d.geometry.PointCloud): The point cloud

        Returns
            grasp_points ([[geometry_msgs.msg.Point, distance_to_ground]]):
                The best grasp points found for each stalk, and their distance to the ground plane
        '''
        # Find the ground plane
        plane = cls.ransac_ground_plane(pointcloud)

        # Find the point closest to 2" from the ground plane for each stalk
        # NOTE: This relies on the ground plane being horizontal. For non-horizontal, use point to plane distance
        goal_height = -plane[3] + 2
        grasp_points = []
        for stalk in stalks_features:
            best_point = min(stalk, key=lambda pt, goal_height=goal_height: abs(pt.z - goal_height))
            distance_to_ground = best_point.z - (-plane[3])
            grasp_points.append([best_point, distance_to_ground])

        return grasp_points

    @classmethod
    def combine_pointclouds(cls, pointclouds, transformers):
        '''
        Combine multiple point clouds into one using ICP and the given transformations

        Parameters
            pointclouds (list[o3d.geometry.PointCloud]): The point clouds

        Returns
            pointcloud (o3d.geometry.PointCloud): The combined point cloud
        '''
        # If ICP is needed
        transformations = []
        for pcl, transformer in zip(pointclouds[1:], transformers[1:]):
            # Find the predicted transformations between pcl and the first pointcloud
            trans_init = transformer.E @ np.linalg.inv(transformers[0].E)

            result = o3d.pipelines.registration.registration_icp(
                pcl, pointclouds[0], 0.05, trans_init,
                o3d.pipelines.registration.TransformationEstimationPointToPlane())
            transformations.append(result.transformation)

        # Combine the point clouds
        pointcloud = pointclouds[0]
        for pcl in pointclouds[1:]:
            # Transform the pointcloud to the frame of the first one if ICP is being used
            pcl.transform(transformations.pop())
            pointcloud.points.extend(pcl.points)

    @classmethod
    def run_realsense(cls):
        cls.driver_process = subprocess.Popen(DRIVER_COMMAND, stdout=subprocess.PIPE, shell=True, preexec_fn=os.setsid)

    @classmethod
    def stop_realsense(cls):
        if cls.driver_process is not None:
            os.killpg(os.getpgid(cls.driver_process.pid), signal.SIGTERM)
            cls.driver_process = None

    @classmethod
    def find_stalk(cls, req: GetStalkVisionRequest) -> GetStalkVisionResponse:
        '''
        Process a request to find the best stalk to grab

        Parameters
            req (GetStalkRequest): The request, with a number of frames to process and a timeout

        Returns
            GetStalkResponse: The response, with a message, the best stalk found, and # frames processed
        '''
        print('Received a request for {} frames with timeout {} seconds'.format(req.num_frames, req.timeout))
        cls.call_index += 1

        # Run the realsense camera driver if needed
        if RUN_REALSENSE_ON_REQUEST:
            cls.run_realsense()

        try:
            camera_info = rospy.wait_for_message(CAMERA_INFO, CameraInfo, timeout=2)
            if camera_info is None:
                raise rospy.ROSException
            cls.camera_height = camera_info.height
            cls.camera_width = camera_info.width
        except rospy.ROSException:
            print(colored('Camera info not found, so camera is likely not running!', 'red'))
            return GetStalkVisionResponse(success='ERROR', num_frames=0)

        cls.image_index = -1
        start = rospy.get_rostime()
        positions: list[Point] = []
        pcls = []
        transformers: list[Transformer] = []

        def image_depth_callback(image, depth_image):
            nonlocal positions, pcls, transformers

            transformer = Transformer(cls.tf_buffer.get_tf_buffer())
            cls.image_index += 1

            # Run the model
            masks, output, scores = cls.run_detection(image)

            cv_depth_image = cls.cv_bridge.imgmsg_to_cv2(depth_image, desired_encoding="passthrough")
            cv_depth_image = np.array(cv_depth_image, dtype=np.float32)

            # region Grasp Point Finding
            stalks_features: list[list[Any]] = cls.get_features(masks)

            # Visualize the stalks features on the image
            if VISUALIZE:
                cv_image = cls.cv_bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')
                features_image = cls.model.visualize(cv_image, output).astype(np.uint8)
                for stalk in stalks_features:
                    for point in stalk:
                        try:
                            cv.circle(features_image, (int(point.x), int(point.y)), 2, (0, 0, 255), -1)
                        except Exception as e:
                            print('Unable to put circle in stalk image', e)
                            break
                cls.visualizer.publish_item('masks', features_image)

            # Add the depths to the stalk features
            for i, stalk in enumerate(stalks_features):
                for j in range(len(stalk)):
                    # TODO: Use more pixels from the depth image to get a better depth (only if they are in the mask)

                    # Get the depth from the depth image at this point
                    stalks_features[i][j] = Point(x=cls.camera_width - stalk[j].x, y=cls.camera_height - stalk[j].y,
                                                  z=cv_depth_image[int(stalk[j].y), int(stalk[j].x)] / DEPTH_SCALE)

            # Transform the points
            stalks_features = cls.transform_points_to_world(stalks_features, transformer)

            # Turn these features into stalks
            stalks = []
            for stalk, score, mask in zip(stalks_features, scores, masks):
                new_stalk = Stalk(points=stalk, score=score, mask=mask)

                # Ensure the stalk is valid, in case RANSAC failed
                if new_stalk.is_valid():
                    stalks.append(new_stalk)

            if VISUALIZE:
                cls.visualizer.new_frame()
                cls.visualizer.publish_item('features', stalks_features, marker_color=[255, 0, 0])
                for stalk in stalks:
                    cls.visualizer.publish_item('stalk_lines', stalk, marker_color=[255, 255, 0], marker_size=0.01)

            if GRASP_POINT_ALGO == GraspPointFindingOptions.mask_only:
                for stalk in stalks:
                    stalk.set_grasp_point(min_height=min([p.z for p in stalk.points]))

            elif GRASP_POINT_ALGO == GraspPointFindingOptions.mask_projection:
                for stalk in stalks:
                    stalk.set_grasp_point(min_height=0)

            elif GRASP_POINT_ALGO == GraspPointFindingOptions.ransac_ground_plane:
                pointcloud = cls.get_pcl(image, depth_image, transformer)

                if VISUALIZE:
                    cls.visualizer.publish_item('pointcloud', pointcloud)

                grasp_points = cls.get_grasp_points_by_ransac(stalks_features, pointcloud)

            else:
                raise ValueError('Invalid grasp point finding algorithm')
            # endregion

            # Set the camera grasping points
            for stalk in stalks:
                stalk.set_cam_grasp_point(transformer.transform_world_to_cam_frame(stalk.grasp_point))

            # region Best Stalk Determination
            # Filter out stalks that are not within the bounds of the camera
            valid_stalks: list[Stalk] = [s for s in stalks if s.is_within_bounds()]

            if len(valid_stalks) == 0:
                rospy.logwarn('No valid stalks detected on frame {} ({} total stalks found)'.format(cls.image_index, len(stalks)))
                # print('Stalks are {}'.format([s.cam_grasp_point for s in stalks]))
                cls.stop_realsense()
                return

            if BEST_STALK_ALGO == BestStalkOptions.largest:
                best_stalk = max(valid_stalks, key=lambda stalk: np.count_nonzero(stalk.mask))

            elif BEST_STALK_ALGO == BestStalkOptions.largest_favorable:
                cv.imwrite('masks/{}.png'.format(cls.image_index), features_image)
                print('IMAGE {}: y coords {}'.format(cls.image_index, [s.cam_grasp_point.y for s in valid_stalks]))

                graspable_stalks: list[Stalk] = []

                # Order by grasping point coordinates instead of masks, to account for possible sporadic points
                ordered_right_to_left: list[Stalk] = sorted(valid_stalks, key=lambda stalk: stalk.cam_grasp_point.y)

                # Note that we do not need to worry about stalks far on the right side of the frame, since they'll be eliminated earlier

                # Iterate from the right to the left, eliminating stalks that are too close for the gripper to fit between
                graspable_stalks.append(ordered_right_to_left[0])
                for i in range(1, len(ordered_right_to_left)):
                    is_free = True
                    for j in range(0, i):
                        # NOTE: Temporary
                        assert ordered_right_to_left[i].cam_grasp_point.y >= ordered_right_to_left[j].cam_grasp_point.y

                        if ordered_right_to_left[i].cam_grasp_point.y - ordered_right_to_left[j].cam_grasp_point.y < GRIPPER_WIDTH and \
                                ordered_right_to_left[j].cam_grasp_point.x - ordered_right_to_left[i].cam_grasp_point.x < GRIPPER_LENGTH_PAST_STALK:
                            is_free = False
                            break

                    if is_free:
                        graspable_stalks.append(ordered_right_to_left[i])

                # For the remaining stalks, weight by width, confidence, and height
                stalk_weights = []
                for i, stalk in enumerate(graspable_stalks):
                    # Use the z coordinate in world frame
                    mask_height = np.ptp([p.z for p in stalk.points])

                    # Use the average width of the mask in the image
                    mask = np.swapaxes(stalk.mask, 0, 1)
                    nonzero = np.nonzero(mask)
                    top_y, bottom_y = nonzero[1].min(), nonzero[1].max()
                    widths = []

                    for y in range(top_y, bottom_y, 10):
                        x = nonzero[0][np.where(nonzero[1] == y)]
                        if len(x) > 0:
                            widths.append(np.ptp(x))

                    mask_width = np.mean(widths)

                    # Use the y coordinate in camera frame
                    distance_from_center = abs(stalk.cam_grasp_point.y)

                    weight = (stalk.score ** 2) * mask_width * (math.pow(mask_height, 1/3)) * (1 - distance_from_center)

                    print('Stalk index {} has weight {}, score {}, height {}, width {} distance_from_center {} y {}'.format(
                        i, weight, stalks[i].score, mask_height, mask_width, distance_from_center, stalk.cam_grasp_point.y))

                    stalk_weights.append(weight)

                best_stalk = graspable_stalks[np.argmax(stalk_weights)]

                if VISUALIZE:
                    cls.visualizer.publish_item('grasp_points', [best_stalk.grasp_point], marker_color=[255, 0, 255], marker_size=0.02)

            elif BEST_STALK_ALGO == BestStalkOptions.combine_pcls:
                positions += grasp_points
                pointcloud = cls.get_pcl(image, depth_image, transformer)
                pcls.append(pointcloud)
                transformers.append(transformer)

            else:
                raise ValueError('Invalid best stalk finding algorithm')

            if BEST_STALK_ALGO in [BestStalkOptions.largest, BestStalkOptions.largest_favorable]:
                positions.append(transformer.transform_world_to_cam_frame(best_stalk.grasp_point))
            # endregion

        # Setup the callback for the images and depth images
        cls.sub_images = KillableSubscriber(IMAGE_TOPIC, Image)
        cls.sub_depth = KillableSubscriber(DEPTH_TOPIC, Image)
        ts = ApproximateTimeSynchronizer(
            [cls.sub_images, cls.sub_depth], queue_size=5, slop=0.2)
        ts.registerCallback(image_depth_callback)

        # Continue until enough frames have been gathered, or the timeout has been reached
        while cls.image_index < req.num_frames and (rospy.get_rostime() - start).to_sec() < req.timeout:
            rospy.sleep(0.1)

        # Unregister the callback
        cls.sub_depth.unregister()
        cls.sub_images.unregister()
        del ts

        #  Cluster the positions, find the best one, average it
        if len(positions) == 0:
            print(colored('No valid stalks detected in any frame for this service request, requesting a REPOSITION', 'red'))
            cls.stop_realsense()
            return GetStalkVisionResponse(success='REPOSITION', num_frames=cls.image_index + 1)

        # Option 1 or 2.1: Simply find the consensus among the individual decisions
        if BEST_STALK_ALGO in [BestStalkOptions.largest, BestStalkOptions.largest_favorable]:
            try:
                best_stalk = cls.determine_best_stalk(positions)
            except Exception:
                print(colored('WARNING: Best stalk determination failed, so using only the first frame', 'red'))
                best_stalk = positions[0]

        elif BEST_STALK_ALGO == BestStalkOptions.combine_pcls:
            # Combine point clouds with the first one
            single_pcl = cls.combine_pointclouds(pcls, transformers)

            # Find the grasp points for this point cloud, which is in the frame of the first point cloud
            grasp_points = cls.get_grasp_points_by_ransac(positions, single_pcl)

            best_stalk = cls.determine_best_stalk(grasp_points)

        else:
            raise ValueError('Invalid best stalk algorithm')

        print(colored('Found stalk at robot frame ({}, {}, {})'.format(
            best_stalk.x, best_stalk.y, best_stalk.z), 'green'))

        # Stop the camera driver
        cls.stop_realsense()
        return GetStalkVisionResponse(success='DONE', position=best_stalk, num_frames=cls.image_index + 1)


if __name__ == '__main__':
    print('Starting stalk_detect_vision node.\n\tUsing grasp point algorithm: {}\n\tUsing best stalk algorithm: {}'.format(
        GRASP_POINT_ALGO.name, BEST_STALK_ALGO.name))

    rospy.init_node('stalk_detect_vision')

    detect_node = DetectNode()

    print(colored('Loaded model. Waiting for service calls...', 'green'))

    rospy.spin()
